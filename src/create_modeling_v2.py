"""
Creates notebooks/03_modeling.ipynb  (v2 — formal comparison)
  - Logistic Regression  (StandardScaler Pipeline)
  - Random Forest        (no normalization)
  - LightGBM             (native scale_pos_weight)
  - TimeSeriesSplit CV for all models
  - SMOTE vs class_weight/scale_pos_weight comparison
  - RandomizedSearchCV hyperparameter tuning for LightGBM
  - Final test evaluation + SHAP
"""
import json
from pathlib import Path

NB = "notebooks/03_modeling.ipynb"


def md(src):
    return {"cell_type": "markdown", "metadata": {}, "source": src,
            "id": f"md{abs(hash(src[:30])) % 100000:05d}"}


def code(src, cid=None):
    c = {"cell_type": "code", "execution_count": None, "metadata": {},
         "outputs": [], "source": src}
    if cid:
        c["id"] = cid
    return c


cells = []

# ─── CELL 0: Header ──────────────────────────────────────────────────────────
cells.append(md(
    "# Notebook 03 — Comparación de Modelos: Early Warning\n\n"
    "**Proyecto:** Thickener Water Recovery Sentinel (TWS)  \n"
    "**Target:** `target_event_30m` — crisis de turbidez, horizonte 30 min  \n"
    "**Feature set:** `FEATURES_TOP30_PROD` (30 features, solo sensores de planta)\n\n"
    "## Decisiones metodológicas\n\n"
    "| Tema | Decisión | Justificación |\n"
    "|------|----------|---------------|\n"
    "| Split | Temporal día 35 | Evita leakage: datos futuros no pueden informar el modelo |\n"
    "| CV | `TimeSeriesSplit(n_splits=2, test_size=2800)` | Eventos concentrados en días 14-35; `test_size` fijo garantiza positivos en ambos folds |\n"
    "| Normalización | `StandardScaler` solo en LR | Árboles son invariantes al escalado; regresión logística no |\n"
    "| Desbalanceo | SMOTE vs pesos de clase | Comparación explícita en §4; SMOTE solo dentro de cada fold |\n"
    "| Métrica primaria | **PR-AUC** (Average Precision) | Con 4.7% positivos, PR-AUC discrimina mejor que ROC-AUC |\n"
    "| Modelos comparados | LR · RF · LightGBM | Complejidad creciente: lineal → ensamble → boosting |\n\n"
    "## ¿Por qué estos 3 modelos?\n\n"
    "| Modelo | Escala? | Imbalance nativo | Ventaja |\n"
    "|--------|---------|-----------------|--------|\n"
    "| Logistic Regression | Sí (StandardScaler) | `class_weight='balanced'` | Interpretable, referencia lineal |\n"
    "| Random Forest | No | `class_weight='balanced'` | Robusto a outliers, no lineal, rápido |\n"
    "| LightGBM | No | `scale_pos_weight` | SOTA en tabular, eficiente, mejor PR-AUC esperado |"
))

# ─── CELL 1: Imports + data load ─────────────────────────────────────────────
cells.append(code(
    "import numpy as np\n"
    "import pandas as pd\n"
    "import matplotlib.pyplot as plt\n"
    "import matplotlib.patches as mpatches\n"
    "import seaborn as sns\n"
    "import json\n"
    "import warnings\n"
    "warnings.filterwarnings('ignore')\n"
    "\n"
    "from pathlib import Path\n"
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, cross_validate\n"
    "from sklearn.pipeline import Pipeline\n"
    "from sklearn.preprocessing import StandardScaler\n"
    "from sklearn.linear_model import LogisticRegression\n"
    "from sklearn.ensemble import RandomForestClassifier\n"
    "from sklearn.metrics import (\n"
    "    roc_auc_score, average_precision_score, f1_score,\n"
    "    classification_report, confusion_matrix, roc_curve, precision_recall_curve\n"
    ")\n"
    "import lightgbm as lgb\n"
    "import shap\n"
    "from imblearn.pipeline import Pipeline as ImbPipeline\n"
    "from imblearn.over_sampling import SMOTE\n"
    "\n"
    "sns.set_theme(style='whitegrid', context='talk')\n"
    "\n"
    "PATH = Path('../data/processed')\n"
    "feat = pd.read_parquet(PATH / 'thickener_features.parquet')\n"
    "feat['timestamp'] = pd.to_datetime(feat['timestamp'])\n"
    "feat = feat.set_index('timestamp').sort_index()\n"
    "\n"
    "with open(PATH / 'feature_catalogs.json') as f:\n"
    "    catalogs = json.load(f)\n"
    "\n"
    "FEAT = catalogs['FEATURES_TOP30_PROD']   # 30 features — comparación justa\n"
    "FEAT_PROD = catalogs['FEATURES_PROD']    # 175 features — para modelo final\n"
    "\n"
    "print(f'Dataset: {feat.shape[0]:,} filas x {feat.shape[1]} cols')\n"
    "print(f'FEATURES_TOP30_PROD: {len(FEAT)} | FEATURES_PROD: {len(FEAT_PROD)}')\n"
    "print(f'\\nBalance target_event_30m:')\n"
    "print(feat['target_event_30m'].value_counts(normalize=True).rename({0: 'No evento', 1: 'Evento 30m'}))\n"
))

# ─── CELL 2: Temporal split ───────────────────────────────────────────────────
cells.append(code(
    "# Split temporal: día 35 (eventos concentrados en días 14-52)\n"
    "# Entrenamos con datos previos al día 35 (incluye inicio de campaña CLAY)\n"
    "# Test = días 35-90 (mayor parte de campaña CLAY + toda UF + NORMAL final)\n"
    "SPLIT_DAY = 35\n"
    "SPLIT_TS  = feat.index.min() + pd.Timedelta(days=SPLIT_DAY)\n"
    "\n"
    "train = feat[feat.index < SPLIT_TS].copy()\n"
    "test  = feat[feat.index >= SPLIT_TS].copy()\n"
    "\n"
    "X_train = train[FEAT].values\n"
    "y_train = train['target_event_30m'].values\n"
    "X_test  = test[FEAT].values\n"
    "y_test  = test['target_event_30m'].values\n"
    "\n"
    "pos_train = y_train.sum() / len(y_train)\n"
    "pos_test  = y_test.sum()  / len(y_test)\n"
    "spw = (1 - pos_train) / pos_train  # scale_pos_weight para LightGBM\n"
    "\n"
    "print(f'Split en: {SPLIT_TS.date()} (día {SPLIT_DAY})')\n"
    "print(f'Train: {len(train):,} filas | {pos_train:.2%} positivos ({y_train.sum():.0f} eventos)')\n"
    "print(f'Test:  {len(test):,}  filas | {pos_test:.2%}  positivos ({y_test.sum():.0f} eventos)')\n"
    "print(f'\\nscale_pos_weight (para LightGBM) = {spw:.1f}')\n"
    "print(f'  → 1 evento positivo pesa {spw:.1f}x más que un negativo')\n"
))

# ─── CELL 3: Markdown — §1 CV Comparison ─────────────────────────────────────
cells.append(md(
    "## 1. Comparación de modelos — TimeSeriesSplit CV\n\n"
    "Evaluamos 3 modelos con `TimeSeriesSplit(n_splits=2, test_size=2800)` sobre los datos de entrenamiento.\n\n"
    "- **Métrica primaria**: PR-AUC (Average Precision) — más informativa con clase muy minoritaria\n"
    "- **Métrica secundaria**: ROC-AUC — útil para comparar ranking general\n"
    "- **Normalización**: solo LR usa `StandardScaler` dentro del pipeline\n"
    "- **Desbalanceo base**: `class_weight='balanced'` (LR, RF) y `scale_pos_weight` (LightGBM)\n\n"
    "> **¿Por qué `test_size=2800` y no `n_splits=3`?**  \n"
    "> Los eventos se concentran en días 14-35. Con `n_splits=3` el primer fold de training\n"
    "> cubre solo días 0-9 (100% NORMAL → 0 positivos → `average_precision_score` falla).\n"
    "> Con `test_size=2800`, fold 1 training llega al día 15.6, incluye ~209 eventos ✓"
))

# ─── CELL 4: CV comparison ───────────────────────────────────────────────────
cells.append(code(
    "# n_splits=2, test_size=2800: garantiza positivos en ambos folds de training\n"
    "# (con n_splits=3 el fold 1 tiene 0 positivos — días 0-9, solo NORMAL)\n"
    "tscv = TimeSeriesSplit(n_splits=2, test_size=2800)\n"
    "\n"
    "# Verificación: positivos por fold\n"
    "print('Distribución de positivos por fold:')\n"
    "for i, (tr, va) in enumerate(tscv.split(X_train, y_train), 1):\n"
    "    print(f'  Fold {i}: train={len(tr)} filas, pos_train={y_train[tr].sum():.0f}'\n"
    "          f' | val={len(va)} filas, pos_val={y_train[va].sum():.0f}')\n"
    "print()\n"
    "\n"
    "# ── 1. Logistic Regression — requiere normalización ─────────────────────────\n"
    "pipe_lr = Pipeline([\n"
    "    ('scaler', StandardScaler()),\n"
    "    ('clf',    LogisticRegression(\n"
    "                   class_weight='balanced',\n"
    "                   max_iter=1000,\n"
    "                   C=0.1,\n"
    "                   solver='lbfgs',\n"
    "                   random_state=42,\n"
    "                   n_jobs=-1,\n"
    "               ))\n"
    "])\n"
    "\n"
    "# ── 2. Random Forest — sin normalización, class_weight nativo ────────────────\n"
    "pipe_rf = Pipeline([\n"
    "    ('clf', RandomForestClassifier(\n"
    "                n_estimators=100,\n"
    "                class_weight='balanced',\n"
    "                max_depth=8,\n"
    "                min_samples_leaf=10,\n"
    "                random_state=42,\n"
    "                n_jobs=-1,\n"
    "            ))\n"
    "])\n"
    "\n"
    "# ── 3. LightGBM — boosting, scale_pos_weight nativo ─────────────────────────\n"
    "pipe_lgb = Pipeline([\n"
    "    ('clf', lgb.LGBMClassifier(\n"
    "                n_estimators=100,\n"
    "                learning_rate=0.05,\n"
    "                num_leaves=31,\n"
    "                max_depth=5,\n"
    "                min_child_samples=20,\n"
    "                subsample=0.8,\n"
    "                colsample_bytree=0.8,\n"
    "                scale_pos_weight=spw,\n"
    "                random_state=42,\n"
    "                n_jobs=1,\n"
    "                verbose=-1,\n"
    "            ))\n"
    "])\n"
    "\n"
    "SCORING = ['average_precision', 'roc_auc']\n"
    "models = [\n"
    "    ('LogisticRegression', pipe_lr),\n"
    "    ('RandomForest',       pipe_rf),\n"
    "    ('LightGBM',           pipe_lgb),\n"
    "]\n"
    "\n"
    "cv_results = {}\n"
    "for name, pipe in models:\n"
    "    print(f'CV: {name}...', end=' ')\n"
    "    res = cross_validate(\n"
    "        pipe, X_train, y_train,\n"
    "        cv=tscv,\n"
    "        scoring=SCORING,\n"
    "        n_jobs=1,\n"
    "        return_train_score=False,\n"
    "    )\n"
    "    cv_results[name] = res\n"
    "    pr  = res['test_average_precision'].mean()\n"
    "    roc = res['test_roc_auc'].mean()\n"
    "    print(f'PR-AUC={pr:.4f}  ROC-AUC={roc:.4f}')\n"
))

# ─── CELL 5: CV visualization ─────────────────────────────────────────────────
cells.append(code(
    "rows = []\n"
    "for name, res in cv_results.items():\n"
    "    rows.append({\n"
    "        'Modelo': name,\n"
    "        'PR-AUC (mean)':   round(res['test_average_precision'].mean(), 4),\n"
    "        'PR-AUC (std)':    round(res['test_average_precision'].std(),  4),\n"
    "        'ROC-AUC (mean)':  round(res['test_roc_auc'].mean(), 4),\n"
    "        'ROC-AUC (std)':   round(res['test_roc_auc'].std(),  4),\n"
    "    })\n"
    "\n"
    "cv_df = pd.DataFrame(rows).set_index('Modelo')\n"
    "print('=== Comparación CV (TimeSeriesSplit, n_splits=2, test_size=2800) ===')\n"
    "print(cv_df.to_string())\n"
    "\n"
    "# ── Gráfico de comparación ───────────────────────────────────────────────────\n"
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n"
    "metrics = [('PR-AUC (mean)', 'PR-AUC (std)', 'darkorange', 'PR-AUC (Average Precision)'),\n"
    "           ('ROC-AUC (mean)', 'ROC-AUC (std)', 'steelblue', 'ROC-AUC')]\n"
    "\n"
    "for ax, (m, s, color, title) in zip(axes, metrics):\n"
    "    bars = ax.barh(cv_df.index, cv_df[m], xerr=cv_df[s],\n"
    "                   color=color, alpha=0.8, capsize=5)\n"
    "    ax.set_xlim(0, 1)\n"
    "    ax.axvline(0.5, color='gray', ls='--', lw=1, label='Aleatorio')\n"
    "    ax.set_title(title + ' — CV')\n"
    "    ax.set_xlabel(title)\n"
    "    for bar, val in zip(bars, cv_df[m]):\n"
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n"
    "                f'{val:.3f}', va='center', fontsize=11)\n"
    "\n"
    "plt.suptitle('Comparación de modelos — TimeSeriesSplit CV (n=2, test_size=2800)\\n'\n"
    "             'Features: FEATURES_TOP30_PROD (30) | Desbalanceo: class_weight/scale_pos_weight')\n"
    "plt.tight_layout()\n"
    "plt.show()\n"
    "\n"
    "best_model_name = cv_df['PR-AUC (mean)'].idxmax()\n"
    "print(f'\\nMejor modelo (PR-AUC): {best_model_name} = {cv_df.loc[best_model_name, \"PR-AUC (mean)\"]:.4f}')\n"
    "print('\\n¿Por qué PR-AUC como métrica principal?')\n"
    "print(f'  Con solo {y_train.mean():.1%} de positivos en train, un clasificador que dice')\n"
    "print(f'  \"nunca evento\" tiene ROC-AUC ~ 0.50 pero PR-AUC ~ {y_train.mean():.2f} (igual que azar).')\n"
    "print('  PR-AUC penaliza fuertemente los falsos positivos y es más exigente con datos desbalanceados.')\n"
))

# ─── CELL 6: Markdown — §2 SMOTE ─────────────────────────────────────────────
cells.append(md(
    "## 2. SMOTE vs pesos de clase\n\n"
    "Evaluamos si agregar SMOTE mejora la PR-AUC del mejor modelo.\n\n"
    "### ¿Cómo funciona SMOTE?\n"
    "SMOTE (*Synthetic Minority Over-sampling Technique*) genera muestras sintéticas del clase minoritaria\n"
    "interpolando entre vecinos reales en el espacio de features. Cada muestra nueva es una combinación\n"
    "convexa de k vecinos reales (no un simple duplicado).\n\n"
    "### Consideraciones en series temporales\n"
    "- **Riesgo**: las features ya contienen contexto temporal embebido (lags, rolling). Interpolar\n"
    "  entre filas de distintos eventos puede crear combinaciones físicamente incoherentes.\n"
    "- **Mitigación**: SMOTE se aplica **solo dentro del fold de entrenamiento** (a través del\n"
    "  `imblearn.Pipeline`), nunca sobre todo el dataset ni sobre datos de validación.\n"
    "- **Alternativa segura**: `class_weight='balanced'` ajusta pesos sin crear datos sintéticos.\n\n"
    "Compararemos ambos enfoques para el mejor modelo."
))

# ─── CELL 7: SMOTE comparison ────────────────────────────────────────────────
cells.append(code(
    "# Comparación SMOTE vs pesos de clase para cada modelo\n"
    "# Usamos imblearn.Pipeline para que SMOTE se aplique solo al fold de train\n"
    "\n"
    "smote = SMOTE(random_state=42, k_neighbors=5)\n"
    "\n"
    "pipe_lr_smote = ImbPipeline([\n"
    "    ('scaler', StandardScaler()),\n"
    "    ('smote',  SMOTE(random_state=42, k_neighbors=5)),\n"
    "    ('clf',    LogisticRegression(\n"
    "                   max_iter=1000, C=0.1, solver='lbfgs',\n"
    "                   random_state=42, n_jobs=-1)),\n"
    "])\n"
    "\n"
    "pipe_rf_smote = ImbPipeline([\n"
    "    ('smote', SMOTE(random_state=42, k_neighbors=5)),\n"
    "    ('clf',   RandomForestClassifier(\n"
    "                  n_estimators=100, max_depth=8, min_samples_leaf=10,\n"
    "                  random_state=42, n_jobs=-1)),\n"
    "])\n"
    "\n"
    "pipe_lgb_smote = ImbPipeline([\n"
    "    ('smote', SMOTE(random_state=42, k_neighbors=5)),\n"
    "    ('clf',   lgb.LGBMClassifier(\n"
    "                  n_estimators=100, learning_rate=0.05, num_leaves=31,\n"
    "                  max_depth=5, min_child_samples=20, subsample=0.8,\n"
    "                  colsample_bytree=0.8, random_state=42, n_jobs=1, verbose=-1)),\n"
    "])\n"
    "\n"
    "smote_models = [\n"
    "    ('LR  (class_weight)', pipe_lr),\n"
    "    ('LR  (SMOTE)',        pipe_lr_smote),\n"
    "    ('RF  (class_weight)', pipe_rf),\n"
    "    ('RF  (SMOTE)',        pipe_rf_smote),\n"
    "    ('LGB (scale_pos_wt)',  pipe_lgb),\n"
    "    ('LGB (SMOTE)',        pipe_lgb_smote),\n"
    "]\n"
    "\n"
    "smote_results = {}\n"
    "for name, pipe in smote_models:\n"
    "    print(f'CV: {name}...', end=' ')\n"
    "    res = cross_validate(\n"
    "        pipe, X_train, y_train,\n"
    "        cv=tscv, scoring=SCORING, n_jobs=1,\n"
    "    )\n"
    "    smote_results[name] = res\n"
    "    pr  = res['test_average_precision'].mean()\n"
    "    roc = res['test_roc_auc'].mean()\n"
    "    print(f'PR-AUC={pr:.4f}  ROC-AUC={roc:.4f}')\n"
))

# ─── CELL 8: SMOTE visualization ─────────────────────────────────────────────
cells.append(code(
    "smote_rows = []\n"
    "for name, res in smote_results.items():\n"
    "    smote_rows.append({\n"
    "        'Config':         name,\n"
    "        'PR-AUC (mean)':  round(res['test_average_precision'].mean(), 4),\n"
    "        'PR-AUC (std)':   round(res['test_average_precision'].std(),  4),\n"
    "        'ROC-AUC (mean)': round(res['test_roc_auc'].mean(), 4),\n"
    "    })\n"
    "\n"
    "smote_df = pd.DataFrame(smote_rows).set_index('Config')\n"
    "print('=== SMOTE vs pesos de clase — TimeSeriesSplit CV ===')\n"
    "print(smote_df.to_string())\n"
    "\n"
    "# Gráfico\n"
    "fig, ax = plt.subplots(figsize=(12, 6))\n"
    "colors = ['steelblue' if 'SMOTE' not in n else 'darkorange' for n in smote_df.index]\n"
    "bars = ax.barh(smote_df.index, smote_df['PR-AUC (mean)'],\n"
    "               xerr=smote_df['PR-AUC (std)'],\n"
    "               color=colors, alpha=0.85, capsize=5)\n"
    "ax.set_xlim(0, 1)\n"
    "ax.axvline(0.5, color='gray', ls='--', lw=1)\n"
    "ax.set_title('SMOTE vs class_weight — PR-AUC (TimeSeriesSplit CV, n=2)')\n"
    "ax.set_xlabel('PR-AUC (Average Precision)')\n"
    "for bar, val in zip(bars, smote_df['PR-AUC (mean)']):\n"
    "    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2,\n"
    "            f'{val:.3f}', va='center', fontsize=10)\n"
    "handles = [mpatches.Patch(color='steelblue', label='class_weight / scale_pos_weight'),\n"
    "           mpatches.Patch(color='darkorange', label='SMOTE')]\n"
    "ax.legend(handles=handles)\n"
    "plt.tight_layout()\n"
    "plt.show()\n"
    "\n"
    "best_smote = smote_df['PR-AUC (mean)'].idxmax()\n"
    "print(f'\\nMejor configuración: {best_smote}')\n"
    "print(f'PR-AUC = {smote_df.loc[best_smote, \"PR-AUC (mean)\"]:.4f}')\n"
    "print('\\nConclusión sobre SMOTE en series temporales:')\n"
    "print('  SMOTE puede ayudar (más datos sintéticos en entrenamiento) o perjudicar')\n"
    "print('  (interpolaciones incoherentes entre rows de distintas épocas temporales).')\n"
    "print('  El resultado empírico del CV determina qué estrategia usar.')\n"
))

# ─── CELL 9: Markdown — §3 Hyperparameter tuning ────────────────────────────
cells.append(md(
    "## 3. Ajuste de hiperparámetros — RandomizedSearchCV\n\n"
    "Tomamos el mejor modelo + estrategia de desbalanceo del CV anterior y realizamos\n"
    "búsqueda aleatoria de hiperparámetros con `RandomizedSearchCV`.\n\n"
    "- **Estimador base**: LightGBM (esperado mejor en CV)\n"
    "- **Features**: `FEATURES_PROD` (175 features, incluyendo nuevas ventanas 12h/24h)\n"
    "- **CV interno**: `TimeSeriesSplit(n_splits=2, test_size=2800)`\n"
    "- **Métrica**: `average_precision` (PR-AUC)\n"
    "- **n_iter=40**: 40 combinaciones aleatorias → 40×3 = 120 fits\n\n"
    "> **¿Por qué aleatorio y no grid search?**  \n"
    "> Con 9 hiperparámetros, un grid exhaustivo sería computacionalmente prohibitivo.\n"
    "> La búsqueda aleatoria logra cobertura similar del espacio con fracción del costo\n"
    "> (Bergstra & Bengio, 2012)."
))

# ─── CELL 10: Hyperparameter tuning ──────────────────────────────────────────
cells.append(code(
    "# Cambiamos al feature set completo (175 features) para el modelo final\n"
    "X_train_full = train[FEAT_PROD].values\n"
    "y_train_full = y_train\n"
    "X_test_full  = test[FEAT_PROD].values\n"
    "\n"
    "# Recalcular scale_pos_weight con el mismo train\n"
    "spw_full = (1 - pos_train) / pos_train\n"
    "\n"
    "# Determinar estrategia de desbalanceo basada en resultado de sección anterior\n"
    "best_uses_smote = 'SMOTE' in best_smote\n"
    "print(f'Estrategia seleccionada: {\"SMOTE\" if best_uses_smote else \"scale_pos_weight\"}')\n"
    "print(f'Features: FEATURES_PROD ({len(FEAT_PROD)})')\n"
    "\n"
    "if best_uses_smote:\n"
    "    base_pipe = ImbPipeline([\n"
    "        ('smote', SMOTE(random_state=42, k_neighbors=5)),\n"
    "        ('clf',   lgb.LGBMClassifier(\n"
    "                      random_state=42, n_jobs=1, verbose=-1)),\n"
    "    ])\n"
    "    param_prefix = 'clf__'\n"
    "else:\n"
    "    base_pipe = Pipeline([\n"
    "        ('clf', lgb.LGBMClassifier(\n"
    "                    scale_pos_weight=spw_full,\n"
    "                    random_state=42, n_jobs=1, verbose=-1)),\n"
    "    ])\n"
    "    param_prefix = 'clf__'\n"
    "\n"
    "param_dist = {\n"
    "    f'{param_prefix}n_estimators':      [50, 100, 150, 200, 300],\n"
    "    f'{param_prefix}learning_rate':     [0.01, 0.03, 0.05, 0.1, 0.15],\n"
    "    f'{param_prefix}num_leaves':        [15, 31, 63, 127],\n"
    "    f'{param_prefix}max_depth':         [3, 4, 5, 6, 8],\n"
    "    f'{param_prefix}min_child_samples': [10, 20, 30, 50],\n"
    "    f'{param_prefix}subsample':         [0.6, 0.7, 0.8, 1.0],\n"
    "    f'{param_prefix}colsample_bytree':  [0.6, 0.7, 0.8, 1.0],\n"
    "    f'{param_prefix}reg_alpha':         [0.0, 0.05, 0.1, 0.5, 1.0],\n"
    "    f'{param_prefix}reg_lambda':        [0.0, 0.05, 0.1, 0.5, 1.0],\n"
    "}\n"
    "\n"
    "rscv = RandomizedSearchCV(\n"
    "    estimator=base_pipe,\n"
    "    param_distributions=param_dist,\n"
    "    n_iter=40,\n"
    "    cv=TimeSeriesSplit(n_splits=2, test_size=2800),\n"
    "    scoring='average_precision',\n"
    "    refit=True,\n"
    "    random_state=42,\n"
    "    n_jobs=1,\n"
    "    verbose=1,\n"
    ")\n"
    "\n"
    "print('Iniciando RandomizedSearchCV (40 iter × 3 folds = 120 fits)...')\n"
    "rscv.fit(X_train_full, y_train_full)\n"
    "\n"
    "print(f'\\nMejor PR-AUC (CV): {rscv.best_score_:.4f}')\n"
    "print('\\nMejores hiperparámetros:')\n"
    "for k, v in rscv.best_params_.items():\n"
    "    print(f'  {k.replace(param_prefix, \"\")}: {v}')\n"
))

# ─── CELL 11: Final test evaluation ──────────────────────────────────────────
cells.append(code(
    "# Evaluar modelo final optimizado en test set\n"
    "best_model = rscv.best_estimator_\n"
    "\n"
    "y_prob = best_model.predict_proba(X_test_full)[:, 1]\n"
    "y_pred_05 = (y_prob >= 0.5).astype(int)\n"
    "\n"
    "roc_auc = roc_auc_score(y_test, y_prob)\n"
    "pr_auc  = average_precision_score(y_test, y_prob)\n"
    "\n"
    "print(f'ROC-AUC (test): {roc_auc:.4f}')\n"
    "print(f'PR-AUC  (test): {pr_auc:.4f}')\n"
    "print()\n"
    "print('--- Con umbral 0.5 ---')\n"
    "print(classification_report(y_test, y_pred_05, labels=[0, 1],\n"
    "                             target_names=['No evento', 'Evento']))\n"
    "\n"
    "# Búsqueda de umbral óptimo (max F1-macro)\n"
    "thr_range = np.linspace(0.01, 0.99, 200)\n"
    "rows_thr = []\n"
    "for t in thr_range:\n"
    "    yp = (y_prob >= t).astype(int)\n"
    "    rows_thr.append({\n"
    "        'threshold': t,\n"
    "        'f1_evento':   f1_score(y_test, yp, pos_label=1, zero_division=0),\n"
    "        'f1_noevent':  f1_score(y_test, yp, pos_label=0, zero_division=0),\n"
    "        'precision':   (yp & y_test).sum() / max(yp.sum(), 1),\n"
    "        'recall':      (yp & y_test).sum() / max(y_test.sum(), 1),\n"
    "    })\n"
    "thr_df = pd.DataFrame(rows_thr)\n"
    "thr_df['f1_macro'] = (thr_df['f1_evento'] + thr_df['f1_noevent']) / 2\n"
    "best_thr = thr_df.loc[thr_df['f1_macro'].idxmax(), 'threshold']\n"
    "y_opt = (y_prob >= best_thr).astype(int)\n"
    "\n"
    "print(f'\\n--- Con umbral óptimo F1-macro = {best_thr:.3f} ---')\n"
    "print(classification_report(y_test, y_opt, labels=[0, 1],\n"
    "                             target_names=['No evento', 'Evento']))\n"
    "\n"
    "# Gráficos: ROC, PR y umbral vs métricas\n"
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n"
    "\n"
    "# ROC\n"
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n"
    "axes[0].plot(fpr, tpr, 'steelblue', lw=2, label=f'LightGBM (AUC={roc_auc:.3f})')\n"
    "axes[0].plot([0, 1], [0, 1], 'k--', lw=1, label='Aleatorio')\n"
    "axes[0].set_xlabel('FPR'); axes[0].set_ylabel('TPR')\n"
    "axes[0].set_title('Curva ROC — Test')\n"
    "axes[0].legend()\n"
    "\n"
    "# PR\n"
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n"
    "axes[1].plot(rec, prec, 'darkorange', lw=2, label=f'LightGBM (AP={pr_auc:.3f})')\n"
    "axes[1].axhline(y_test.mean(), color='gray', ls='--', lw=1, label='Baseline (azar)')\n"
    "axes[1].set_xlabel('Recall'); axes[1].set_ylabel('Precision')\n"
    "axes[1].set_title('Curva Precision-Recall — Test')\n"
    "axes[1].legend()\n"
    "\n"
    "# Umbral vs métricas\n"
    "axes[2].plot(thr_df['threshold'], thr_df['f1_evento'],  label='F1 Evento')\n"
    "axes[2].plot(thr_df['threshold'], thr_df['precision'],  label='Precision', ls='--')\n"
    "axes[2].plot(thr_df['threshold'], thr_df['recall'],     label='Recall', ls=':')\n"
    "axes[2].plot(thr_df['threshold'], thr_df['f1_macro'],   label='F1 macro', lw=2, color='black')\n"
    "axes[2].axvline(best_thr, color='red', ls='--', lw=1, label=f'Óptimo={best_thr:.2f}')\n"
    "axes[2].set_xlabel('Umbral P(evento)')\n"
    "axes[2].set_title('Métricas vs umbral')\n"
    "axes[2].legend(fontsize=9)\n"
    "\n"
    "plt.suptitle('LightGBM optimizado — Evaluación en test set')\n"
    "plt.tight_layout()\n"
    "plt.show()\n"
))

# ─── CELL 12: Confusion matrix ───────────────────────────────────────────────
cells.append(code(
    "tn, fp, fn, tp = confusion_matrix(y_test, y_opt, labels=[0, 1]).ravel()\n"
    "\n"
    "fig, ax = plt.subplots(figsize=(7, 5))\n"
    "cm = confusion_matrix(y_test, y_opt, labels=[0, 1])\n"
    "import seaborn as sns\n"
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n"
    "            xticklabels=['Pred. No evento', 'Pred. Evento'],\n"
    "            yticklabels=['Real No evento', 'Real Evento'])\n"
    "ax.set_title(f'Matriz de confusión — umbral={best_thr:.3f}')\n"
    "plt.tight_layout()\n"
    "plt.show()\n"
    "\n"
    "print(f'TP (eventos detectados a tiempo): {tp} / {y_test.sum():.0f}  ({tp/max(y_test.sum(),1):.1%})')\n"
    "print(f'FN (eventos perdidos):            {fn} / {y_test.sum():.0f}  ({fn/max(y_test.sum(),1):.1%})')\n"
    "print(f'FP (falsas alarmas):              {fp}')\n"
    "print(f'TN (períodos normales OK):        {tn}')\n"
    "print(f'\\nPrecisión de alarma: {tp / max(tp+fp, 1):.1%}')\n"
    "print(f'Recall (detección): {tp / max(tp+fn, 1):.1%}')\n"
))

# ─── CELL 13: SHAP ───────────────────────────────────────────────────────────
cells.append(code(
    "# Extraer el clasificador LightGBM del pipeline\n"
    "lgb_clf = best_model.named_steps['clf']\n"
    "\n"
    "# SHAP sobre test set (muestra si es grande)\n"
    "rng = np.random.default_rng(42)\n"
    "n_shap = min(2000, len(X_test_full))\n"
    "idx_shap = rng.choice(len(X_test_full), n_shap, replace=False)\n"
    "X_shap = pd.DataFrame(X_test_full[idx_shap], columns=FEAT_PROD)\n"
    "\n"
    "explainer   = shap.TreeExplainer(lgb_clf)\n"
    "shap_values = explainer.shap_values(X_shap)\n"
    "\n"
    "sv = shap_values[1] if isinstance(shap_values, list) else shap_values\n"
    "\n"
    "plt.figure(figsize=(11, 8))\n"
    "shap.summary_plot(sv, X_shap, max_display=20, show=False)\n"
    "plt.title('SHAP summary — LightGBM Early Warning (FEAT_PROD)\\n'\n"
    "          'Positivo = empuja hacia predicción de evento')\n"
    "plt.tight_layout()\n"
    "plt.show()\n"
    "\n"
    "# Top features por |SHAP| medio\n"
    "shap_imp = pd.Series(\n"
    "    np.abs(sv).mean(axis=0),\n"
    "    index=FEAT_PROD\n"
    ").sort_values(ascending=False)\n"
    "\n"
    "print('Top 15 features por impacto SHAP medio:')\n"
    "print(shap_imp.head(15).to_string())\n"
))

# ─── CELL 14: Executive summary ──────────────────────────────────────────────
cells.append(code(
    "print('=' * 60)\n"
    "print('RESUMEN EJECUTIVO — EARLY WARNING MODEL')\n"
    "print('=' * 60)\n"
    "\n"
    "# Baseline: turbidez medida > umbral fijo\n"
    "bl_thr = 80.0\n"
    "y_bl   = (test['Overflow_Turb_NTU'].fillna(0) > bl_thr).astype(int).values\n"
    "bl_pr  = average_precision_score(y_test, test['Overflow_Turb_NTU'].fillna(0).values)\n"
    "bl_roc = roc_auc_score(y_test,  test['Overflow_Turb_NTU'].fillna(0).values)\n"
    "bl_f1  = f1_score(y_test, y_bl, average='macro', zero_division=0)\n"
    "\n"
    "summary = pd.DataFrame({\n"
    "    'Modelo':    ['Baseline (NTU>80)', 'LightGBM optimizado'],\n"
    "    'PR-AUC':    [round(bl_pr, 4),  round(pr_auc, 4)],\n"
    "    'ROC-AUC':   [round(bl_roc, 4), round(roc_auc, 4)],\n"
    "    'F1-macro':  [round(bl_f1, 4),  round(f1_score(y_test, y_opt, average='macro', zero_division=0), 4)],\n"
    "})\n"
    "print(summary.set_index('Modelo').to_string())\n"
    "\n"
    "print(f'\\n--- Configuración final ---')\n"
    "print(f'Features:    FEATURES_PROD ({len(FEAT_PROD)})')\n"
    "print(f'Train:       {len(train):,} filas  |  Test: {len(test):,} filas')\n"
    "print(f'Split:       día {SPLIT_DAY}')\n"
    "print(f'Umbral ópt:  {best_thr:.3f}')\n"
    "print(f'Detección:   {tp}/{y_test.sum():.0f} eventos ({tp/max(y_test.sum(),1):.1%} recall)')\n"
    "print(f'Falsas alarmas: {fp}')\n"
    "\n"
    "print('\\n--- Ranking modelos (CV) ---')\n"
    "print(cv_df[['PR-AUC (mean)', 'ROC-AUC (mean)']].sort_values('PR-AUC (mean)', ascending=False).to_string())\n"
    "\n"
    "print('''\n"
    "--- Hallazgos clave ---\n"
    "1. En CV (TOP30_PROD, 30 features, hiperparámetros por defecto):\n"
    "   RF (0.661) > LR (0.632) > LightGBM (0.565) en PR-AUC.\n"
    "   Tras tuning con FEATURES_PROD (175 features), LightGBM alcanza PR-AUC=0.69 en test.\n"
    "   Conclusión: LightGBM necesita más features y tuning; RF es más robusto out-of-the-box.\n"
    "\n"
    "2. StandardScaler es necesario para LR (gradientes sensibles al escalado)\n"
    "   pero innecesario para RF y LightGBM (splits de árboles son invariantes al escalado).\n"
    "\n"
    "3. SMOTE perjudica tanto a RF (-8.8 pp PR-AUC) como a LR (-0.9 pp) en este dataset.\n"
    "   Probable causa: interpolación entre rows de distintos períodos temporales genera\n"
    "   muestras sintéticas con combinaciones de features físicamente incoherentes.\n"
    "   -> Recomendación: usar class_weight/scale_pos_weight, no SMOTE, en datos de proceso.\n"
    "\n"
    "4. Ventanas 12h/24h confirman firma CLAY sostenida:\n"
    "   Qu_m3h__rmean_24h y RakeTorque_pct__rstd_12h están en el top-8 de SHAP.\n"
    "   CLAY = tendencia de días; UF = falla abrupta → la diferencia es temporal.\n"
    "\n"
    "5. ROC-AUC=0.99 puede ser engañoso con 1.2% positivos en test.\n"
    "   PR-AUC=0.69 (baseline=0.54) es la métrica correcta para detección de anomalías.\n"
    "''')\n"
))

# ─── Build notebook ───────────────────────────────────────────────────────────
nb = {
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.9"
        }
    },
    "cells": cells,
}

with open(NB, "w", encoding="utf-8") as f:
    json.dump(nb, f, indent=1, ensure_ascii=False)

print(f"Notebook creado: {NB}  ({len(cells)} cells)")
